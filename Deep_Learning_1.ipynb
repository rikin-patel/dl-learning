{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8d6244",
   "metadata": {},
   "source": [
    "# FEED FORWARD NEURAL NETWORK (MLP - Multilayer Perceptron)\n",
    "\n",
    "**Modified code to use Intel XPU**\n",
    "**More can be found here: https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-to-intel-extension-for-tensorflow.html**\n",
    "**https://intel.github.io/intel-extension-for-tensorflow/latest/docs/install/install_for_xpu.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade intel-extension-for-tensorflow[xpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import intel_extension_for_tensorflow as itex\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TenserFlow executint eagerly: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Fashion MNIST Dataset and split into training and test sets\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_and_validation_images, train_and_validation_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "text_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct a validation set from the training set\n",
    "validation_images = train_and_validation_images[-10000:, :, :]\n",
    "validation_labels = train_and_validation_labels[-10000:]\n",
    "training_images = train_and_validation_images[:-50000, :, :]\n",
    "training_labels = train_and_validation_labels[:-50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the data\n",
    "plt.figure()\n",
    "random_index = np.random.randint(0, len(training_images))\n",
    "plt.imshow(training_images[random_index], cmap='gray_r')\n",
    "plt.colorbar()\n",
    "numerical_label = training_labels[random_index]\n",
    "text_description = text_labels[numerical_label]\n",
    "plt.title('Label: {} (\"{}\")'.format(numerical_label, text_description))\n",
    "plt.gca().grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ee26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Showing 25 random images from the training set\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    random_index = np.random.randint(0, len(training_images))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(training_images[random_index], cmap='gray_r')\n",
    "    numerical_label = training_labels[random_index]\n",
    "    text_description = text_labels[numerical_label]\n",
    "    plt.title(text_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  ### Hyperparameter: batch size\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_images, training_labels))\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.cast(y, tf.int32)))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=batch_size * 10)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.cast(y, tf.int32)))\n",
    "validation_dataset = validation_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb61d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28) , name='flatten_input_layer'),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu, name='first_hidden_layer'),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu, name='second_hidden_layer'),\n",
    "    tf.keras.layers.Dense(10, name='hidden_to_logits')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb412972",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the Model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "num_epochs = 50  ### Hyperparameter: number of epochs\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4015495",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to train the model using tf.GradientTape\n",
    "@tf.function\n",
    "def train_step(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(image)\n",
    "        loss = loss_function(label, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93586fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to evaluate the model on validation data\n",
    "@tf.function\n",
    "def val_step(image, label):\n",
    "    logits = model(image, training=False)\n",
    "    loss = loss_function(label, logits)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(label, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344a7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35420048",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for val_images, val_labels in validation_dataset:\n",
    "        val_step(val_images, val_labels)\n",
    "\n",
    "    template = 'Epoch {:03d}, Loss: {:.03f}, Acc: {:.3%}, Val Loss: {:.03f}, Val Acc: {:.3%}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result(),\n",
    "                          val_loss.result(),\n",
    "                          val_accuracy.result()))\n",
    "\n",
    "    train_losses.append(train_loss.result())\n",
    "    train_accuracies.append(train_accuracy.result())\n",
    "    \n",
    "    val_losses.append(val_loss.result())\n",
    "    val_accuracies.append(val_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b361b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot training and validation loss/accuracy over epochs --- Answer -1\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning behavior analysis and validation vs test comparison ---- Answer -1\n",
    "final_train_loss = float(train_losses[-1])\n",
    "final_train_acc = float(train_accuracies[-1])\n",
    "final_val_loss = float(val_losses[-1])\n",
    "final_val_acc = float(val_accuracies[-1])\n",
    "\n",
    "gap_acc = final_train_acc - final_val_acc\n",
    "gap_loss = final_val_loss - final_train_loss\n",
    "\n",
    "print('Model WITHOUT Dropout:')\n",
    "print('Final epoch:')\n",
    "print(f'  Train Loss: {final_train_loss:.4f}, Train Acc: {final_train_acc:.2%}')\n",
    "print(f'  Val   Loss: {final_val_loss:.4f}, Val   Acc: {final_val_acc:.2%}')\n",
    "print(f'  Generalization gap (Acc): {gap_acc:.2%}, (Loss): {gap_loss:.4f}')\n",
    "\n",
    "# Store baseline results for comparison\n",
    "baseline_train_losses = train_losses.copy()\n",
    "baseline_train_accuracies = train_accuracies.copy()\n",
    "baseline_val_losses = val_losses.copy()\n",
    "baseline_val_accuracies = val_accuracies.copy()\n",
    "baseline_final_train_acc = final_train_acc\n",
    "baseline_final_val_acc = final_val_acc\n",
    "baseline_gap_acc = gap_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataset))\n",
    "\n",
    "_logits = model(images, training=False)\n",
    "predictions = tf.argmax(_logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "img_indexes = np.arange(images.numpy().shape[0])\n",
    "np.random.shuffle(img_indexes)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = img_indexes[i]\n",
    "    predicted_label = int(predictions[img_index])\n",
    "    plt.imshow(images[img_index], cmap='gray_r')\n",
    "    true_label = labels[img_index].numpy()\n",
    "    predicted_label = predictions[img_index].numpy()\n",
    "    color = 'blue' if true_label == predicted_label else 'red'\n",
    "    plt.title('TRUE: {}\\nPREDICTED: {}'.format(text_labels[true_label],\n",
    "                                    text_labels[predicted_label]),\n",
    "                                    color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Model on the test dataset\n",
    "tf_test_images = tf.convert_to_tensor(test_images, dtype=tf.float32) / 255.0\n",
    "tf_test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdca8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = model(tf_test_images, training=False)\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "t_loss = loss_function(tf_test_labels, test_logits)\n",
    "test_loss(t_loss)\n",
    "\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "test_accuracy(tf_test_labels, test_logits)\n",
    "\n",
    "print('Test Loss: {:.03f}, Test Accuracy: {:.3%}'.format(test_loss.result(),\n",
    "                                                         test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = tf.argmax(test_logits, axis=1, output_type=tf.int32)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(test_images[i], cmap='gray_r')\n",
    "    true_label = test_labels[i]\n",
    "    predicted_label = test_predictions[i].numpy()\n",
    "    color = 'blue' if true_label == predicted_label else 'red'\n",
    "    plt.title('TRUE: {}\\nPREDICTED: {}'.format(text_labels[true_label],\n",
    "                                    text_labels[predicted_label]),\n",
    "                                    color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765a9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eba849ce",
   "metadata": {},
   "source": [
    "## Model with Dropout Layers --- Answer - 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cad73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Model WITH Dropout layers\n",
    "model_dropout = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten_input_layer'),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu, name='first_hidden_layer'),\n",
    "    tf.keras.layers.Dropout(0.3, name='dropout_1'),  # Dropout layer with 30% dropout rate\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu, name='second_hidden_layer'),\n",
    "    tf.keras.layers.Dropout(0.3, name='dropout_2'),  # Dropout layer with 30% dropout rate\n",
    "    tf.keras.layers.Dense(10, name='hidden_to_logits')\n",
    "])\n",
    "\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863759a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup training for model WITH Dropout\n",
    "optimizer_dropout = tf.keras.optimizers.Adam()\n",
    "loss_function_dropout = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_loss_dropout = tf.keras.metrics.Mean(name='train_loss_dropout')\n",
    "train_accuracy_dropout = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy_dropout')\n",
    "train_losses_dropout = []\n",
    "train_accuracies_dropout = []\n",
    "\n",
    "val_loss_dropout = tf.keras.metrics.Mean(name='val_loss_dropout')\n",
    "val_accuracy_dropout = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy_dropout')\n",
    "val_losses_dropout = []\n",
    "val_accuracies_dropout = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training and validation functions for dropout model\n",
    "@tf.function\n",
    "def train_step_dropout(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model_dropout(image, training=True)  # training=True enables dropout\n",
    "        loss = loss_function_dropout(label, logits)\n",
    "    gradients = tape.gradient(loss, model_dropout.trainable_variables)\n",
    "    optimizer_dropout.apply_gradients(zip(gradients, model_dropout.trainable_variables))\n",
    "\n",
    "    train_loss_dropout(loss)\n",
    "    train_accuracy_dropout(label, logits)\n",
    "\n",
    "@tf.function\n",
    "def val_step_dropout(image, label):\n",
    "    logits = model_dropout(image, training=False)  # training=False disables dropout\n",
    "    loss = loss_function_dropout(label, logits)\n",
    "\n",
    "    val_loss_dropout(loss)\n",
    "    val_accuracy_dropout(label, logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model WITH Dropout\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step_dropout(images, labels)\n",
    "\n",
    "    for val_images, val_labels in validation_dataset:\n",
    "        val_step_dropout(val_images, val_labels)\n",
    "\n",
    "    template = 'Epoch {:03d}, Loss: {:.03f}, Acc: {:.3%}, Val Loss: {:.03f}, Val Acc: {:.3%}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss_dropout.result(),\n",
    "                          train_accuracy_dropout.result(),\n",
    "                          val_loss_dropout.result(),\n",
    "                          val_accuracy_dropout.result()))\n",
    "\n",
    "    train_losses_dropout.append(train_loss_dropout.result())\n",
    "    train_accuracies_dropout.append(train_accuracy_dropout.result())\n",
    "    val_losses_dropout.append(val_loss_dropout.result())\n",
    "    val_accuracies_dropout.append(val_accuracy_dropout.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison plots: Model WITHOUT Dropout vs Model WITH Dropout\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Loss comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, baseline_train_losses, 'b-', label='Train (No Dropout)', alpha=0.7)\n",
    "plt.plot(epochs, baseline_val_losses, 'b--', label='Val (No Dropout)', alpha=0.7)\n",
    "plt.plot(epochs, train_losses_dropout, 'r-', label='Train (With Dropout)', alpha=0.7)\n",
    "plt.plot(epochs, val_losses_dropout, 'r--', label='Val (With Dropout)', alpha=0.7)\n",
    "plt.title('Loss Comparison: Without vs With Dropout')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, baseline_train_accuracies, 'b-', label='Train (No Dropout)', alpha=0.7)\n",
    "plt.plot(epochs, baseline_val_accuracies, 'b--', label='Val (No Dropout)', alpha=0.7)\n",
    "plt.plot(epochs, train_accuracies_dropout, 'r-', label='Train (With Dropout)', alpha=0.7)\n",
    "plt.plot(epochs, val_accuracies_dropout, 'r--', label='Val (With Dropout)', alpha=0.7)\n",
    "plt.title('Accuracy Comparison: Without vs With Dropout')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quantitative comparison and analysis --- Answer - 2\n",
    "print('=' * 70)\n",
    "print('COMPARISON: Model WITHOUT Dropout vs Model WITH Dropout')\n",
    "print('=' * 70)\n",
    "\n",
    "# Without Dropout\n",
    "final_train_loss_no_dropout = float(baseline_train_losses[-1])\n",
    "final_train_acc_no_dropout = float(baseline_train_accuracies[-1])\n",
    "final_val_loss_no_dropout = float(baseline_val_losses[-1])\n",
    "final_val_acc_no_dropout = float(baseline_val_accuracies[-1])\n",
    "gap_acc_no_dropout = final_train_acc_no_dropout - final_val_acc_no_dropout\n",
    "gap_loss_no_dropout = final_val_loss_no_dropout - final_train_loss_no_dropout\n",
    "\n",
    "# With Dropout\n",
    "final_train_loss_dropout = float(train_losses_dropout[-1])\n",
    "final_train_acc_dropout = float(train_accuracies_dropout[-1])\n",
    "final_val_loss_dropout = float(val_losses_dropout[-1])\n",
    "final_val_acc_dropout = float(val_accuracies_dropout[-1])\n",
    "gap_acc_dropout = final_train_acc_dropout - final_val_acc_dropout\n",
    "gap_loss_dropout = final_val_loss_dropout - final_train_loss_dropout\n",
    "\n",
    "print('\\nModel WITHOUT Dropout:')\n",
    "print(f'  Train Acc: {final_train_acc_no_dropout:.2%}, Val Acc: {final_val_acc_no_dropout:.2%}')\n",
    "print(f'  Train Loss: {final_train_loss_no_dropout:.4f}, Val Loss: {final_val_loss_no_dropout:.4f}')\n",
    "print(f'  Generalization Gap (Acc): {gap_acc_no_dropout:.2%}, (Loss): {gap_loss_no_dropout:.4f}')\n",
    "\n",
    "print('\\nModel WITH Dropout:')\n",
    "print(f'  Train Acc: {final_train_acc_dropout:.2%}, Val Acc: {final_val_acc_dropout:.2%}')\n",
    "print(f'  Train Loss: {final_train_loss_dropout:.4f}, Val Loss: {final_val_loss_dropout:.4f}')\n",
    "print(f'  Generalization Gap (Acc): {gap_acc_dropout:.2%}, (Loss): {gap_loss_dropout:.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('IMPROVEMENT ANALYSIS')\n",
    "print('=' * 70)\n",
    "\n",
    "improvement_gap_acc = gap_acc_no_dropout - gap_acc_dropout\n",
    "improvement_gap_loss = gap_loss_no_dropout - gap_loss_dropout\n",
    "improvement_val_acc = final_val_acc_dropout - final_val_acc_no_dropout\n",
    "\n",
    "print(f'\\nGeneralization Gap Reduction:')\n",
    "print(f'  Accuracy gap reduced by: {improvement_gap_acc:.2%}')\n",
    "print(f'  Loss gap reduced by: {improvement_gap_loss:.4f}')\n",
    "print(f'\\nValidation Performance Change:')\n",
    "print(f'  Validation accuracy change: {improvement_val_acc:+.2%}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('HOW DROPOUT AFFECTS OVERFITTING')\n",
    "print('=' * 70)\n",
    "print('\\nDropout is a regularization technique that:')\n",
    "print('  1. Randomly deactivates neurons during training (30% in our case)')\n",
    "print('  2. Forces the network to learn robust features')\n",
    "print('  3. Prevents co-adaptation of neurons')\n",
    "print('  4. Acts as an ensemble method')\n",
    "\n",
    "if gap_acc_dropout < gap_acc_no_dropout:\n",
    "    print('\\n✓ Dropout REDUCED overfitting:')\n",
    "    print(f'    - Generalization gap decreased from {gap_acc_no_dropout:.2%} to {gap_acc_dropout:.2%}')\n",
    "    print(f'    - Training-validation gap is now smaller')\n",
    "    if final_val_acc_dropout > final_val_acc_no_dropout:\n",
    "        print(f'    - Validation accuracy improved by {improvement_val_acc:.2%}')\n",
    "    else:\n",
    "        print(f'    - Slight trade-off: validation accuracy changed by {improvement_val_acc:.2%}')\n",
    "        print('      (This is acceptable as the model generalizes better)')\n",
    "else:\n",
    "    print('\\n⚠ Unexpected: Dropout did not reduce the generalization gap.')\n",
    "    print('   Consider: adjusting dropout rate, training longer, or other regularization.')\n",
    "\n",
    "print('\\nObservations from the plots:')\n",
    "print('  • Training accuracy should be lower with dropout (neurons randomly disabled)')\n",
    "print('  • Validation accuracy should be closer to training accuracy')\n",
    "print('  • The gap between training and validation curves should be smaller')\n",
    "print('  • Model with dropout is less prone to overfitting the training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e476e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Dropout model on test dataset\n",
    "test_logits_dropout = model_dropout(tf_test_images, training=False)\n",
    "test_loss_dropout = tf.keras.metrics.Mean(name='test_loss_dropout')\n",
    "t_loss_dropout = loss_function_dropout(tf_test_labels, test_logits_dropout)\n",
    "test_loss_dropout(t_loss_dropout)\n",
    "\n",
    "test_accuracy_dropout = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy_dropout')\n",
    "test_accuracy_dropout(tf_test_labels, test_logits_dropout)\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('TEST SET PERFORMANCE COMPARISON')\n",
    "print('=' * 70)\n",
    "print(f'\\nModel WITHOUT Dropout - Test Acc: {test_accuracy.result():.2%}, Test Loss: {test_loss.result():.4f}')\n",
    "print(f'Model WITH Dropout    - Test Acc: {test_accuracy_dropout.result():.2%}, Test Loss: {test_loss_dropout.result():.4f}')\n",
    "print(f'\\nTest Accuracy Improvement: {(test_accuracy_dropout.result() - test_accuracy.result()):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062951e",
   "metadata": {},
   "source": [
    "## Model with Batch Normalization --- Answer - 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Model WITH Batch Normalization layers\n",
    "model_batchnorm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten_input_layer'),\n",
    "    tf.keras.layers.Dense(256, activation=None, name='first_hidden_layer'),  # No activation here\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_1'),  # Batch Normalization layer\n",
    "    tf.keras.layers.Activation(tf.nn.relu, name='relu_1'),  # Activation after BatchNorm\n",
    "    tf.keras.layers.Dense(128, activation=None, name='second_hidden_layer'),  # No activation here\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_2'),  # Batch Normalization layer\n",
    "    tf.keras.layers.Activation(tf.nn.relu, name='relu_2'),  # Activation after BatchNorm\n",
    "    tf.keras.layers.Dense(10, name='hidden_to_logits')\n",
    "])\n",
    "\n",
    "model_batchnorm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup training for model WITH Batch Normalization\n",
    "optimizer_batchnorm = tf.keras.optimizers.Adam()\n",
    "loss_function_batchnorm = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_loss_batchnorm = tf.keras.metrics.Mean(name='train_loss_batchnorm')\n",
    "train_accuracy_batchnorm = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy_batchnorm')\n",
    "train_losses_batchnorm = []\n",
    "train_accuracies_batchnorm = []\n",
    "\n",
    "val_loss_batchnorm = tf.keras.metrics.Mean(name='val_loss_batchnorm')\n",
    "val_accuracy_batchnorm = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy_batchnorm')\n",
    "val_losses_batchnorm = []\n",
    "val_accuracies_batchnorm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training and validation functions for batch normalization model\n",
    "@tf.function\n",
    "def train_step_batchnorm(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model_batchnorm(image, training=True)  # training=True updates BN statistics\n",
    "        loss = loss_function_batchnorm(label, logits)\n",
    "    gradients = tape.gradient(loss, model_batchnorm.trainable_variables)\n",
    "    optimizer_batchnorm.apply_gradients(zip(gradients, model_batchnorm.trainable_variables))\n",
    "\n",
    "    train_loss_batchnorm(loss)\n",
    "    train_accuracy_batchnorm(label, logits)\n",
    "\n",
    "@tf.function\n",
    "def val_step_batchnorm(image, label):\n",
    "    logits = model_batchnorm(image, training=False)  # training=False uses running stats\n",
    "    loss = loss_function_batchnorm(label, logits)\n",
    "\n",
    "    val_loss_batchnorm(loss)\n",
    "    val_accuracy_batchnorm(label, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model WITH Batch Normalization\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step_batchnorm(images, labels)\n",
    "\n",
    "    for val_images, val_labels in validation_dataset:\n",
    "        val_step_batchnorm(val_images, val_labels)\n",
    "\n",
    "    template = 'Epoch {:03d}, Loss: {:.03f}, Acc: {:.3%}, Val Loss: {:.03f}, Val Acc: {:.3%}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss_batchnorm.result(),\n",
    "                          train_accuracy_batchnorm.result(),\n",
    "                          val_loss_batchnorm.result(),\n",
    "                          val_accuracy_batchnorm.result()))\n",
    "\n",
    "    train_losses_batchnorm.append(train_loss_batchnorm.result())\n",
    "    train_accuracies_batchnorm.append(train_accuracy_batchnorm.result())\n",
    "    val_losses_batchnorm.append(val_loss_batchnorm.result())\n",
    "    val_accuracies_batchnorm.append(val_accuracy_batchnorm.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081677e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comprehensive comparison plots: All three models\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Training Loss Comparison\n",
    "axes[0, 0].plot(epochs, baseline_train_losses, 'b-', label='No Regularization', alpha=0.8, linewidth=2)\n",
    "axes[0, 0].plot(epochs, train_losses_dropout, 'r-', label='With Dropout', alpha=0.8, linewidth=2)\n",
    "axes[0, 0].plot(epochs, train_losses_batchnorm, 'g-', label='With Batch Norm', alpha=0.8, linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss Comparison\n",
    "axes[0, 1].plot(epochs, baseline_val_losses, 'b--', label='No Regularization', alpha=0.8, linewidth=2)\n",
    "axes[0, 1].plot(epochs, val_losses_dropout, 'r--', label='With Dropout', alpha=0.8, linewidth=2)\n",
    "axes[0, 1].plot(epochs, val_losses_batchnorm, 'g--', label='With Batch Norm', alpha=0.8, linewidth=2)\n",
    "axes[0, 1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy Comparison\n",
    "axes[1, 0].plot(epochs, baseline_train_accuracies, 'b-', label='No Regularization', alpha=0.8, linewidth=2)\n",
    "axes[1, 0].plot(epochs, train_accuracies_dropout, 'r-', label='With Dropout', alpha=0.8, linewidth=2)\n",
    "axes[1, 0].plot(epochs, train_accuracies_batchnorm, 'g-', label='With Batch Norm', alpha=0.8, linewidth=2)\n",
    "axes[1, 0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Comparison\n",
    "axes[1, 1].plot(epochs, baseline_val_accuracies, 'b--', label='No Regularization', alpha=0.8, linewidth=2)\n",
    "axes[1, 1].plot(epochs, val_accuracies_dropout, 'r--', label='With Dropout', alpha=0.8, linewidth=2)\n",
    "axes[1, 1].plot(epochs, val_accuracies_batchnorm, 'g--', label='With Batch Norm', alpha=0.8, linewidth=2)\n",
    "axes[1, 1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convergence Speed Analysis --- Answer - 3\n",
    "import numpy as np\n",
    "\n",
    "print('=' * 80)\n",
    "print('CONVERGENCE SPEED ANALYSIS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Define target accuracy thresholds\n",
    "target_train_acc = 0.85\n",
    "target_val_acc = 0.80\n",
    "\n",
    "# Function to find epoch where threshold is first reached\n",
    "def find_convergence_epoch(accuracies, threshold):\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        if float(acc) >= threshold:\n",
    "            return i + 1  # Return 1-indexed epoch\n",
    "    return None  # Threshold not reached\n",
    "\n",
    "# Training convergence\n",
    "train_conv_baseline = find_convergence_epoch(baseline_train_accuracies, target_train_acc)\n",
    "train_conv_dropout = find_convergence_epoch(train_accuracies_dropout, target_train_acc)\n",
    "train_conv_batchnorm = find_convergence_epoch(train_accuracies_batchnorm, target_train_acc)\n",
    "\n",
    "# Validation convergence\n",
    "val_conv_baseline = find_convergence_epoch(baseline_val_accuracies, target_val_acc)\n",
    "val_conv_dropout = find_convergence_epoch(val_accuracies_dropout, target_val_acc)\n",
    "val_conv_batchnorm = find_convergence_epoch(val_accuracies_batchnorm, target_val_acc)\n",
    "\n",
    "print(f'\\nEpochs to reach {target_train_acc:.0%} training accuracy:')\n",
    "print(f'  No Regularization:  {train_conv_baseline if train_conv_baseline else \"Not reached\"} epochs')\n",
    "print(f'  With Dropout:       {train_conv_dropout if train_conv_dropout else \"Not reached\"} epochs')\n",
    "print(f'  With Batch Norm:    {train_conv_batchnorm if train_conv_batchnorm else \"Not reached\"} epochs')\n",
    "\n",
    "print(f'\\nEpochs to reach {target_val_acc:.0%} validation accuracy:')\n",
    "print(f'  No Regularization:  {val_conv_baseline if val_conv_baseline else \"Not reached\"} epochs')\n",
    "print(f'  With Dropout:       {val_conv_dropout if val_conv_dropout else \"Not reached\"} epochs')\n",
    "print(f'  With Batch Norm:    {val_conv_batchnorm if val_conv_batchnorm else \"Not reached\"} epochs')\n",
    "\n",
    "# Early epochs learning rate (first 10 epochs)\n",
    "early_epochs = 10\n",
    "baseline_early_improvement = float(baseline_train_accuracies[early_epochs-1]) - float(baseline_train_accuracies[0])\n",
    "dropout_early_improvement = float(train_accuracies_dropout[early_epochs-1]) - float(train_accuracies_dropout[0])\n",
    "batchnorm_early_improvement = float(train_accuracies_batchnorm[early_epochs-1]) - float(train_accuracies_batchnorm[0])\n",
    "\n",
    "print(f'\\nAccuracy improvement in first {early_epochs} epochs:')\n",
    "print(f'  No Regularization:  {baseline_early_improvement:.2%}')\n",
    "print(f'  With Dropout:       {dropout_early_improvement:.2%}')\n",
    "print(f'  With Batch Norm:    {batchnorm_early_improvement:.2%}')\n",
    "\n",
    "if batchnorm_early_improvement > baseline_early_improvement:\n",
    "    speedup = (batchnorm_early_improvement / baseline_early_improvement - 1) * 100\n",
    "    print(f'\\n✓ Batch Normalization converges {speedup:.1f}% faster in early epochs!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Stability Analysis --- Answer - 3\n",
    "print('\\n' + '=' * 80)\n",
    "print('TRAINING STABILITY ANALYSIS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Calculate loss variance across epochs (indicator of stability)\n",
    "baseline_train_loss_std = np.std([float(x) for x in baseline_train_losses])\n",
    "dropout_train_loss_std = np.std([float(x) for x in train_losses_dropout])\n",
    "batchnorm_train_loss_std = np.std([float(x) for x in train_losses_batchnorm])\n",
    "\n",
    "baseline_val_loss_std = np.std([float(x) for x in baseline_val_losses])\n",
    "dropout_val_loss_std = np.std([float(x) for x in val_losses_dropout])\n",
    "batchnorm_val_loss_std = np.std([float(x) for x in val_losses_batchnorm])\n",
    "\n",
    "print('\\nTraining Loss Standard Deviation (lower = more stable):')\n",
    "print(f'  No Regularization:  {baseline_train_loss_std:.4f}')\n",
    "print(f'  With Dropout:       {dropout_train_loss_std:.4f}')\n",
    "print(f'  With Batch Norm:    {batchnorm_train_loss_std:.4f}')\n",
    "\n",
    "print('\\nValidation Loss Standard Deviation (lower = more stable):')\n",
    "print(f'  No Regularization:  {baseline_val_loss_std:.4f}')\n",
    "print(f'  With Dropout:       {dropout_val_loss_std:.4f}')\n",
    "print(f'  With Batch Norm:    {batchnorm_val_loss_std:.4f}')\n",
    "\n",
    "# Calculate smoothness (difference between consecutive epochs)\n",
    "def calculate_smoothness(losses):\n",
    "    differences = [abs(float(losses[i+1]) - float(losses[i])) for i in range(len(losses)-1)]\n",
    "    return np.mean(differences)\n",
    "\n",
    "baseline_smoothness = calculate_smoothness(baseline_train_losses)\n",
    "dropout_smoothness = calculate_smoothness(train_losses_dropout)\n",
    "batchnorm_smoothness = calculate_smoothness(train_losses_batchnorm)\n",
    "\n",
    "print('\\nTraining Loss Smoothness (lower = more stable):')\n",
    "print(f'  No Regularization:  {baseline_smoothness:.4f}')\n",
    "print(f'  With Dropout:       {dropout_smoothness:.4f}')\n",
    "print(f'  With Batch Norm:    {batchnorm_smoothness:.4f}')\n",
    "\n",
    "if batchnorm_smoothness < baseline_smoothness:\n",
    "    improvement = (1 - batchnorm_smoothness / baseline_smoothness) * 100\n",
    "    print(f'\\n✓ Batch Normalization training is {improvement:.1f}% smoother!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c92c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation Accuracy and Generalization Comparison --- Answer - 3\n",
    "print('\\n' + '=' * 80)\n",
    "print('VALIDATION ACCURACY AND GENERALIZATION COMPARISON')\n",
    "print('=' * 80)\n",
    "\n",
    "# Final performance metrics\n",
    "final_train_acc_batchnorm = float(train_accuracies_batchnorm[-1])\n",
    "final_val_acc_batchnorm = float(val_accuracies_batchnorm[-1])\n",
    "final_train_loss_batchnorm = float(train_losses_batchnorm[-1])\n",
    "final_val_loss_batchnorm = float(val_losses_batchnorm[-1])\n",
    "\n",
    "gap_acc_batchnorm = final_train_acc_batchnorm - final_val_acc_batchnorm\n",
    "gap_loss_batchnorm = final_val_loss_batchnorm - final_train_loss_batchnorm\n",
    "\n",
    "print('\\n--- Model WITHOUT Regularization ---')\n",
    "print(f'  Train Acc: {baseline_final_train_acc:.2%}, Val Acc: {baseline_final_val_acc:.2%}')\n",
    "print(f'  Generalization Gap: {baseline_gap_acc:.2%}')\n",
    "\n",
    "print('\\n--- Model WITH Dropout ---')\n",
    "print(f'  Train Acc: {final_train_acc_dropout:.2%}, Val Acc: {final_val_acc_dropout:.2%}')\n",
    "print(f'  Generalization Gap: {gap_acc_dropout:.2%}')\n",
    "\n",
    "print('\\n--- Model WITH Batch Normalization ---')\n",
    "print(f'  Train Acc: {final_train_acc_batchnorm:.2%}, Val Acc: {final_val_acc_batchnorm:.2%}')\n",
    "print(f'  Generalization Gap: {gap_acc_batchnorm:.2%}')\n",
    "\n",
    "# Best validation accuracy\n",
    "print('\\nValidation Accuracy Ranking:')\n",
    "models_val_acc = [\n",
    "    ('No Regularization', baseline_final_val_acc),\n",
    "    ('Dropout', final_val_acc_dropout),\n",
    "    ('Batch Normalization', final_val_acc_batchnorm)\n",
    "]\n",
    "models_val_acc_sorted = sorted(models_val_acc, key=lambda x: x[1], reverse=True)\n",
    "for i, (name, acc) in enumerate(models_val_acc_sorted):\n",
    "    print(f'  {i+1}. {name}: {acc:.2%}')\n",
    "\n",
    "# Generalization gap ranking (lower is better)\n",
    "print('\\nGeneralization Gap Ranking (lower is better):')\n",
    "models_gap = [\n",
    "    ('No Regularization', baseline_gap_acc),\n",
    "    ('Dropout', gap_acc_dropout),\n",
    "    ('Batch Normalization', gap_acc_batchnorm)\n",
    "]\n",
    "models_gap_sorted = sorted(models_gap, key=lambda x: x[1])\n",
    "for i, (name, gap) in enumerate(models_gap_sorted):\n",
    "    print(f'  {i+1}. {name}: {gap:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comprehensive Analysis: How Batch Normalization Affects Training --- Answer - 3\n",
    "print('\\n' + '=' * 80)\n",
    "print('HOW BATCH NORMALIZATION AFFECTS NEURAL NETWORK TRAINING')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\n1. CONVERGENCE SPEED:')\n",
    "print('   Batch Normalization normalizes inputs to each layer, which:')\n",
    "print('   • Reduces internal covariate shift')\n",
    "print('   • Allows higher learning rates to be used safely')\n",
    "print('   • Accelerates training, especially in early epochs')\n",
    "if train_conv_batchnorm and train_conv_baseline:\n",
    "    if train_conv_batchnorm < train_conv_baseline:\n",
    "        print(f'   ✓ Result: Reached target accuracy {train_conv_baseline - train_conv_batchnorm} epochs faster')\n",
    "\n",
    "print('\\n2. TRAINING STABILITY:')\n",
    "print('   Batch Normalization provides:')\n",
    "print('   • More stable gradient flow through the network')\n",
    "print('   • Reduced sensitivity to weight initialization')\n",
    "print('   • Smoother loss curves with less variance')\n",
    "if batchnorm_smoothness < baseline_smoothness:\n",
    "    improvement = (1 - batchnorm_smoothness / baseline_smoothness) * 100\n",
    "    print(f'   ✓ Result: {improvement:.1f}% reduction in training oscillations')\n",
    "\n",
    "print('\\n3. REGULARIZATION EFFECT:')\n",
    "print('   Batch Normalization acts as a regularizer:')\n",
    "print('   • Adds noise through mini-batch statistics')\n",
    "print('   • Reduces dependence on exact weight values')\n",
    "print('   • Can reduce overfitting (similar to dropout)')\n",
    "if gap_acc_batchnorm < baseline_gap_acc:\n",
    "    improvement = baseline_gap_acc - gap_acc_batchnorm\n",
    "    print(f'   ✓ Result: Generalization gap reduced by {improvement:.2%}')\n",
    "\n",
    "print('\\n4. VALIDATION ACCURACY:')\n",
    "print('   Batch Normalization often improves validation performance by:')\n",
    "print('   • Better generalization through regularization')\n",
    "print('   • More robust learned representations')\n",
    "print('   • Reduced overfitting to training data')\n",
    "if final_val_acc_batchnorm > baseline_final_val_acc:\n",
    "    improvement = final_val_acc_batchnorm - baseline_final_val_acc\n",
    "    print(f'   ✓ Result: Validation accuracy improved by {improvement:.2%}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('KEY DIFFERENCES: Batch Normalization vs Dropout')\n",
    "print('=' * 80)\n",
    "print('\\nBatch Normalization:')\n",
    "print('  • Normalizes layer inputs during both training and inference')\n",
    "print('  • Speeds up convergence by stabilizing learning')\n",
    "print('  • Reduces internal covariate shift')\n",
    "print('  • Can allow higher learning rates')\n",
    "print('  • Has a regularization side-effect')\n",
    "\n",
    "print('\\nDropout:')\n",
    "print('  • Randomly drops neurons during training only')\n",
    "print('  • Primarily a regularization technique')\n",
    "print('  • Forces redundancy in learned representations')\n",
    "print('  • May slow down convergence')\n",
    "print('  • Stronger regularization effect')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('RECOMMENDATIONS')\n",
    "print('=' * 80)\n",
    "print('\\nBased on the analysis:')\n",
    "if final_val_acc_batchnorm >= max(baseline_final_val_acc, final_val_acc_dropout):\n",
    "    print('✓ Batch Normalization achieved the best validation accuracy')\n",
    "    print('  → Recommended for this architecture and dataset')\n",
    "    print('  → Consider combining with Dropout for even better regularization')\n",
    "else:\n",
    "    print('• Batch Normalization improved convergence speed and stability')\n",
    "    print('  → Consider combining techniques or tuning hyperparameters')\n",
    "\n",
    "print('\\nFor optimal results:')\n",
    "print('  1. Use Batch Normalization for faster, more stable training')\n",
    "print('  2. Add Dropout if stronger regularization is needed')\n",
    "print('  3. Experiment with both techniques combined')\n",
    "print('  4. Monitor validation performance to avoid overfitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c80d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Batch Normalization model on test dataset\n",
    "test_logits_batchnorm = model_batchnorm(tf_test_images, training=False)\n",
    "test_loss_batchnorm = tf.keras.metrics.Mean(name='test_loss_batchnorm')\n",
    "t_loss_batchnorm = loss_function_batchnorm(tf_test_labels, test_logits_batchnorm)\n",
    "test_loss_batchnorm(t_loss_batchnorm)\n",
    "\n",
    "test_accuracy_batchnorm = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy_batchnorm')\n",
    "test_accuracy_batchnorm(tf_test_labels, test_logits_batchnorm)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('FINAL TEST SET PERFORMANCE COMPARISON')\n",
    "print('=' * 80)\n",
    "print(f'\\nNo Regularization      - Test Acc: {test_accuracy.result():.2%}, Test Loss: {test_loss.result():.4f}')\n",
    "print(f'With Dropout           - Test Acc: {test_accuracy_dropout.result():.2%}, Test Loss: {test_loss_dropout.result():.4f}')\n",
    "print(f'With Batch Norm        - Test Acc: {test_accuracy_batchnorm.result():.2%}, Test Loss: {test_loss_batchnorm.result():.4f}')\n",
    "\n",
    "# Find best model\n",
    "test_results = [\n",
    "    ('No Regularization', float(test_accuracy.result())),\n",
    "    ('Dropout', float(test_accuracy_dropout.result())),\n",
    "    ('Batch Normalization', float(test_accuracy_batchnorm.result()))\n",
    "]\n",
    "best_model = max(test_results, key=lambda x: x[1])\n",
    "print(f'\\n✓ Best model on test set: {best_model[0]} with {best_model[1]:.2%} accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
